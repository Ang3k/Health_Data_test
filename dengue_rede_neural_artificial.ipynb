{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4b46dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "import sys\n",
    "import pandas as pd\n",
    "\n",
    "print(f'Python: {sys.executable}')\n",
    "print(f'PyTorch: {torch.__version__}')\n",
    "print(f'CUDA built with: {torch.version.cuda}')\n",
    "print(f'CUDA available: {torch.cuda.is_available()}')\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b61eb62",
   "metadata": {},
   "source": [
    "# Processamento de Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6444c158",
   "metadata": {},
   "outputs": [],
   "source": [
    "type_disease = 'zika'  # Change to 'chikungunya' for Chikungunya dataset\n",
    "\n",
    "if type_disease == 'dengue':\n",
    "    df1 = pd.read_csv(\"C:\\\\Users\\\\angej\\\\Documents\\\\2_Programação\\\\Pytorch_Learning\\\\data\\\\DENGBR17.csv\", low_memory=False)\n",
    "    df4 = pd.read_csv(\"C:\\\\Users\\\\angej\\\\Documents\\\\2_Programação\\\\Pytorch_Learning\\\\data\\\\DENGBR18.csv\", low_memory=False)\n",
    "    df2 = pd.read_csv(\"C:\\\\Users\\\\angej\\\\Documents\\\\2_Programação\\\\Pytorch_Learning\\\\data\\\\DENGBR19.csv\", low_memory=False)\n",
    "\n",
    "elif type_disease == 'chikungunya':\n",
    "    df1 = pd.read_csv(\"C:\\\\Users\\\\angej\\\\Documents\\\\2_Programação\\\\Pytorch_Learning\\\\data\\\\CHIKBR17.csv\", low_memory=False)\n",
    "    df4 = pd.read_csv(\"C:\\\\Users\\\\angej\\\\Documents\\\\2_Programação\\\\Pytorch_Learning\\\\data\\\\CHIKBR18.csv\", low_memory=False)\n",
    "    df2 = pd.read_csv(\"C:\\\\Users\\\\angej\\\\Documents\\\\2_Programação\\\\Pytorch_Learning\\\\data\\\\CHIKBR19.csv\", low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0330d0bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([df1, df2, df4], ignore_index=True)\n",
    "\n",
    "# Downcast integer columns to smallest fitting type\n",
    "int_cols = df.select_dtypes(\"int64\").columns\n",
    "df[int_cols] = df[int_cols].apply(pd.to_numeric, downcast=\"integer\")\n",
    "\n",
    "# Downcast float columns\n",
    "float_cols = df.select_dtypes(\"float64\").columns\n",
    "df[float_cols] = df[float_cols].apply(pd.to_numeric, downcast=\"float\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fc72b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.rename(columns={\n",
    "    # === NOTIFICATION INFO ===\n",
    "    'TP_NOT': 'notification_type',          # Type of notification (individual, aggregate, etc.)\n",
    "    'ID_AGRAVO': 'disease_code',            # ICD/SINAN code identifying the disease\n",
    "    'DT_NOTIFIC': 'notification_date',      # Date the case was reported\n",
    "    'SEM_NOT': 'notification_epi_week',     # Epidemiological week of the notification\n",
    "    'NU_ANO': 'notification_year',          # Year the case was reported\n",
    "    'SG_UF_NOT': 'notif_state',            # State (UF) where the case was notified\n",
    "    'ID_MUNICIP': 'notif_municipality',     # Municipality where the case was notified\n",
    "    'ID_REGIONA': 'notif_health_region',    # Health region where the case was notified\n",
    "    'ID_UNIDADE': 'health_facility',        # Health facility that filed the notification\n",
    "    'DT_SIN_PRI': 'symptom_onset_date',     # Date patient first showed symptoms\n",
    "    'SEM_PRI': 'symptom_epi_week',          # Epidemiological week of first symptoms\n",
    "\n",
    "    # === PATIENT DEMOGRAPHICS ===\n",
    "    'ANO_NASC': 'birth_year',              # Patient's year of birth\n",
    "    'NU_IDADE_N': 'age',                   # Patient's age (encoded with unit prefix: days/months/years)\n",
    "    'CS_SEXO': 'sex',                      # Patient's sex (M=male, F=female, I=ignored)\n",
    "    'CS_GESTANT': 'pregnancy_status',       # Pregnancy trimester (1st, 2nd, 3rd) or N/A\n",
    "    'CS_RACA': 'race',                     # Patient's race/ethnicity\n",
    "    'CS_ESCOL_N': 'education_level',        # Patient's education level\n",
    "    'ID_OCUPA_N': 'occupation_code',        # Patient's occupation (CBO code)\n",
    "\n",
    "    # === PATIENT RESIDENCE ===\n",
    "    'SG_UF': 'residence_state',            # State where the patient lives\n",
    "    'ID_MN_RESI': 'residence_municipality', # Municipality where the patient lives\n",
    "    'ID_RG_RESI': 'residence_health_region',# Health region where the patient lives\n",
    "    'ID_PAIS': 'residence_country',         # Country where the patient lives\n",
    "\n",
    "    # === SYMPTOMS (1=Yes, 2=No, 9=Unknown) ===\n",
    "    'FEBRE': 'fever',                       # Fever\n",
    "    'MIALGIA': 'myalgia',                  # Muscle pain\n",
    "    'CEFALEIA': 'headache',                # Headache\n",
    "    'EXANTEMA': 'rash',                    # Skin rash\n",
    "    'VOMITO': 'vomiting',                  # Vomiting\n",
    "    'NAUSEA': 'nausea',                    # Nausea\n",
    "    'DOR_COSTAS': 'back_pain',             # Back pain\n",
    "    'CONJUNTVIT': 'conjunctivitis',        # Conjunctivitis (eye inflammation)\n",
    "    'ARTRITE': 'arthritis',                # Joint inflammation\n",
    "    'ARTRALGIA': 'joint_pain',             # Joint pain\n",
    "    'PETEQUIA_N': 'petechiae',             # Small red/purple spots on skin (bleeding under skin)\n",
    "    'LEUCOPENIA': 'leucopenia',            # Low white blood cell count\n",
    "    'LACO': 'tourniquet_test',             # Tourniquet test (prova do laço) for capillary fragility\n",
    "    'DOR_RETRO': 'retro_orbital_pain',     # Pain behind the eyes\n",
    "\n",
    "    # === COMORBIDITIES (1=Yes, 2=No, 9=Unknown) ===\n",
    "    'DIABETES': 'diabetes',                 # Has diabetes\n",
    "    'HEMATOLOG': 'blood_disorder',          # Has blood/hematological disease\n",
    "    'HEPATOPAT': 'liver_disease',           # Has liver disease\n",
    "    'RENAL': 'kidney_disease',              # Has kidney disease\n",
    "    'HIPERTENSA': 'hypertension',           # Has hypertension\n",
    "    'ACIDO_PEPT': 'peptic_ulcer',           # Has peptic acid disease / ulcer\n",
    "    'AUTO_IMUNE': 'autoimmune_disease',     # Has autoimmune disease\n",
    "\n",
    "    # === CHIKUNGUNYA LAB TESTS ===\n",
    "    'DT_CHIK_S1': 'chik_test1_date',       # Date of Chikungunya serological test 1\n",
    "    'DT_CHIK_S2': 'chik_test2_date',       # Date of Chikungunya serological test 2\n",
    "    'RES_CHIKS1': 'chik_test1_result',      # Result of Chikungunya test 1\n",
    "    'RES_CHIKS2': 'chik_test2_result',      # Result of Chikungunya test 2\n",
    "    'DT_PRNT': 'prnt_date',                # Date of PRNT test (plaque reduction neutralization)\n",
    "    'RESUL_PRNT': 'prnt_result',            # Result of PRNT test\n",
    "\n",
    "    # === DENGUE LAB TESTS ===\n",
    "    'DT_SORO': 'serology_date',            # Date of serological test (IgM)\n",
    "    'RESUL_SORO': 'serology_result',        # Result of serology (positive, negative, inconclusive)\n",
    "    'DT_NS1': 'ns1_test_date',             # Date of NS1 antigen test\n",
    "    'RESUL_NS1': 'ns1_result',             # Result of NS1 test\n",
    "    'DT_VIRAL': 'viral_isolation_date',     # Date of viral isolation test\n",
    "    'RESUL_VI_N': 'viral_isolation_result', # Result of viral isolation\n",
    "    'DT_PCR': 'pcr_date',                  # Date of RT-PCR test\n",
    "    'RESUL_PCR_': 'pcr_result',            # Result of RT-PCR test\n",
    "    'SOROTIPO': 'serotype',                # Dengue serotype identified (DENV-1, 2, 3, or 4)\n",
    "    'HISTOPA_N': 'histopathology',         # Histopathology result\n",
    "    'IMUNOH_N': 'immunohistochemistry',    # Immunohistochemistry result\n",
    "\n",
    "    # === HOSPITALIZATION ===\n",
    "    'HOSPITALIZ': 'hospitalized',           # Whether patient was hospitalized (1=Yes, 2=No)\n",
    "    'DT_INTERNA': 'hospitalization_date',   # Date of hospitalization\n",
    "    'UF': 'hospital_state',                # State of the hospital\n",
    "    'MUNICIPIO': 'hospital_municipality',   # Municipality of the hospital\n",
    "\n",
    "    # === INFECTION ORIGIN ===\n",
    "    'TPAUTOCTO': 'autochthonous_case',      # Whether infection was local or imported\n",
    "    'COUFINF': 'infection_state',           # State where infection likely occurred\n",
    "    'COPAISINF': 'infection_country',       # Country where infection likely occurred\n",
    "    'COMUNINF': 'infection_municipality',   # Municipality where infection likely occurred\n",
    "\n",
    "    # === CLASSIFICATION & OUTCOME ===\n",
    "    'CLASSI_FIN': 'final_classification',   # Final diagnosis (confirmed, discarded, inconclusive)\n",
    "    'CRITERIO': 'confirmation_criteria',    # How it was confirmed (lab, clinical, epidemiological)\n",
    "    'DOENCA_TRA': 'work_related',           # Whether the disease is work-related\n",
    "    'CLINC_CHIK': 'chik_clinical_form',     # Clinical form of Chikungunya (acute, subacute, chronic)\n",
    "    'EVOLUCAO': 'case_outcome',             # Patient outcome (cured, died, etc.)\n",
    "    'DT_OBITO': 'death_date',              # Date of death (if applicable)\n",
    "    'DT_ENCERRA': 'case_closure_date',      # Date the case was officially closed\n",
    "\n",
    "    # === ALARM SIGNS (dengue warning signs, 1=Yes, 2=No) ===\n",
    "    'ALRM_HIPOT': 'alarm_hypotension',     # Postural hypotension (drop in blood pressure)\n",
    "    'ALRM_PLAQ': 'alarm_low_platelets',    # Platelet count drop\n",
    "    'ALRM_VOM': 'alarm_persistent_vomit',  # Persistent vomiting\n",
    "    'ALRM_SANG': 'alarm_bleeding',         # Bleeding from mucous membranes\n",
    "    'ALRM_HEMAT': 'alarm_hematocrit_rise', # Rising hematocrit\n",
    "    'ALRM_ABDOM': 'alarm_abdominal_pain',  # Intense abdominal pain\n",
    "    'ALRM_LETAR': 'alarm_lethargy',        # Lethargy / irritability\n",
    "    'ALRM_HEPAT': 'alarm_liver_enlarged',  # Enlarged liver (hepatomegaly)\n",
    "    'ALRM_LIQ': 'alarm_fluid_accumul',     # Fluid accumulation (pleural effusion, ascites)\n",
    "    'DT_ALRM': 'alarm_signs_date',         # Date alarm signs were observed\n",
    "\n",
    "    # === SEVERITY SIGNS (severe dengue, 1=Yes, 2=No) ===\n",
    "    'GRAV_PULSO': 'severe_weak_pulse',      # Weak or absent pulse\n",
    "    'GRAV_CONV': 'severe_convulsions',      # Convulsions\n",
    "    'GRAV_ENCH': 'severe_cap_refill',       # Slow capillary refill (>2 sec)\n",
    "    'GRAV_INSUF': 'severe_resp_distress',   # Respiratory distress\n",
    "    'GRAV_TAQUI': 'severe_tachycardia',     # Tachycardia (fast heart rate)\n",
    "    'GRAV_EXTRE': 'severe_cold_extremities',# Cold extremities / cyanosis\n",
    "    'GRAV_HIPOT': 'severe_hypotension',     # Hypotension / shock\n",
    "    'GRAV_HEMAT': 'severe_hematemesis',     # Vomiting blood\n",
    "    'GRAV_MELEN': 'severe_melena',          # Black tarry stool (GI bleeding)\n",
    "    'GRAV_METRO': 'severe_metrorrhagia',    # Abnormal uterine bleeding\n",
    "    'GRAV_SANG': 'severe_bleeding',         # Severe bleeding\n",
    "    'GRAV_AST': 'severe_ast_elevated',      # AST/ALT > 1000 (liver enzymes)\n",
    "    'GRAV_MIOC': 'severe_myocarditis',      # Myocarditis (heart inflammation)\n",
    "    'GRAV_CONSC': 'severe_altered_consc',   # Altered consciousness\n",
    "    'GRAV_ORGAO': 'severe_organ_damage',    # Other organ involvement\n",
    "    'DT_GRAV': 'severity_signs_date',       # Date severity signs were observed\n",
    "\n",
    "    # === HEMORRHAGIC MANIFESTATIONS ===\n",
    "    'MANI_HEMOR': 'hemorrhagic_manifest',   # Had hemorrhagic manifestations (1=Yes, 2=No)\n",
    "    'EPISTAXE': 'nosebleed',                # Epistaxis (nosebleed)\n",
    "    'GENGIVO': 'gum_bleeding',              # Gingival bleeding (gums)\n",
    "    'METRO': 'metrorrhagia',                # Abnormal uterine bleeding\n",
    "    'PETEQUIAS': 'petechiae_hemorrh',       # Petechiae (hemorrhagic context)\n",
    "    'HEMATURA': 'hematuria',                # Blood in urine\n",
    "    'SANGRAM': 'other_bleeding',            # Other bleeding\n",
    "    'LACO_N': 'tourniquet_test_hemorrh',    # Tourniquet test (hemorrhagic context)\n",
    "    'PLASMATICO': 'plasma_leakage',         # Evidence of plasma leakage\n",
    "    'EVIDENCIA': 'hemorrhagic_evidence',    # Evidence of hemorrhagic manifestation\n",
    "    'PLAQ_MENOR': 'platelets_below_100k',   # Platelet count < 100,000\n",
    "    'CON_FHD': 'dengue_hemorrhagic_fever',  # Confirmed Dengue Hemorrhagic Fever (DHF)\n",
    "    'COMPLICA': 'complications',            # Complications present\n",
    "\n",
    "    # === ADMINISTRATIVE / SYSTEM ===\n",
    "    'DT_INVEST': 'investigation_date',      # Date the case was investigated\n",
    "    'DT_DIGITA': 'data_entry_date',         # Date the record was entered into the system\n",
    "    'TP_SISTEMA': 'system_type',            # Type of information system used\n",
    "    'NDUPLIC_N': 'duplicate_flag',          # Whether this record is a duplicate\n",
    "    'CS_FLXRET': 'return_flow_flag',        # Case flow return flag (inter-state data sharing)\n",
    "    'FLXRECEBI': 'flow_received',           # Flow received flag (inter-state data sharing)\n",
    "    'MIGRADO_W': 'migrated_from_windows',   # Record migrated from old Windows SINAN system\n",
    "    'DT_NASC' : 'birth_date'                   # Patient's date of birth\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1982775",
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_columns = [\n",
    "    # === ADMINISTRATIVE (no predictive value) ===\n",
    "    'investigation_date',       # filled during/after investigation\n",
    "    'duplicate_flag',           # system control field\n",
    "    'return_flow_flag',         # system control field\n",
    "    'flow_received',            # system control field\n",
    "    'system_type',              # system control field\n",
    "    'notification_type',        # administrative notification type\n",
    "    'notification_epi_week',    # redundant with notification_month/day derived features\n",
    "\n",
    "    # === POST-CLASSIFICATION (filled after or because of final_classification) ===\n",
    "    'confirmation_criteria',    # directly tied to classification (lab, clinical, epidemiological)\n",
    "    'case_closure_date',        # required when classification is filled\n",
    "    'case_outcome',             # outcome recorded after classification (cura, obito, etc.)\n",
    "    'death_date',               # post-outcome\n",
    "    'work_related',             # enabled only if classification=1, cleared if classification=2\n",
    "    'chik_clinical_form',       # required only if classification=13 (Chikungunya)\n",
    "\n",
    "    # === INFECTION ORIGIN (filled only when classification=confirmed, cleared on discard) ===\n",
    "    'autochthonous_case',       # required only if classification=1\n",
    "    'infection_state',          # required only if classification=1\n",
    "    'infection_country',        # required only if classification=1\n",
    "    'infection_municipality',   # required only if classification=1\n",
    "\n",
    "    # === ALARM SIGNS (filled only when classification=11 or 12, leaks the label directly) ===\n",
    "    'alarm_hypotension',\n",
    "    'alarm_low_platelets',\n",
    "    'alarm_persistent_vomit',\n",
    "    'alarm_bleeding',\n",
    "    'alarm_hematocrit_rise',\n",
    "    'alarm_abdominal_pain',\n",
    "    'alarm_lethargy',\n",
    "    'alarm_liver_enlarged',\n",
    "    'alarm_fluid_accumul',\n",
    "    'alarm_signs_date',\n",
    "\n",
    "    # === SEVERITY SIGNS (filled only when classification=12, leaks the label directly) ===\n",
    "    'severe_weak_pulse',\n",
    "    'severe_convulsions',\n",
    "    'severe_cap_refill',\n",
    "    'severe_resp_distress',\n",
    "    'severe_tachycardia',\n",
    "    'severe_cold_extremities',\n",
    "    'severe_hypotension',\n",
    "    'severe_hematemesis',\n",
    "    'severe_melena',\n",
    "    'severe_metrorrhagia',\n",
    "    'severe_bleeding',\n",
    "    'severe_ast_elevated',\n",
    "    'severe_myocarditis',\n",
    "    'severe_altered_consc',\n",
    "    'severe_organ_damage',\n",
    "    'severity_signs_date',\n",
    "\n",
    "    # === DHF / COMPLICATIONS (old classification system, directly informs final_classification) ===\n",
    "    'dengue_hemorrhagic_fever', # confirmed DHF = classification decision\n",
    "    'complications',            # dengue with complications = classification decision\n",
    "\n",
    "    # === CHIKUNGUNYA TESTS (disabled for dengue cases per dictionary, ~97.7% NaN) ===\n",
    "    'chik_test1_date',          # enabled only for Chikungunya, dataset is 100% Dengue (A90)\n",
    "    'chik_test2_date',\n",
    "    'prnt_date',\n",
    "    'chik_test1_result',\n",
    "    'chik_test2_result',\n",
    "    'prnt_result',\n",
    "\n",
    "    # === HOSPITALIZATION (post-assessment decision, not available at diagnosis time) ===\n",
    "    'hospitalized',             # decision made after clinical evaluation\n",
    "    'hospitalization_date',     # only filled if hospitalized\n",
    "    'hospital_state',           # only filled if hospitalized\n",
    "    'hospital_municipality',    # only filled if hospitalized\n",
    "\n",
    "    # === DATE FIELDS (not useful as raw values for ANN, keeping only notification_date, symptom_onset_date, birth_year) ===\n",
    "    'notification_year',        # redundant with notification_date\n",
    "    'serology_date',            # lab test date, not useful as raw value\n",
    "    'ns1_test_date',            # lab test date\n",
    "    'viral_isolation_date',     # lab test date\n",
    "    'pcr_date',                 # lab test date\n",
    "\n",
    "    # === NOT SELF-REPORTABLE (require clinical procedure or lab exam) ===\n",
    "    'leucopenia',               # blood test\n",
    "    'tourniquet_test',          # clinical procedure (prova do laço)\n",
    "    'tourniquet_test_hemorrh',  # clinical procedure (hemorrhagic context)\n",
    "    'plasma_leakage',           # clinical evaluation\n",
    "    'platelets_below_100k',     # blood test\n",
    "    'hemorrhagic_evidence',     # clinical evaluation\n",
    "\n",
    "    # === GEOGRAPHICAL (not available/useful in a self-reported questionnaire) ===\n",
    "    'notif_state',\n",
    "    'notif_municipality',\n",
    "    'notif_health_region',\n",
    "    'health_facility',\n",
    "    'residence_municipality',\n",
    "    'residence_country',\n",
    "]\n",
    "\n",
    "lab_drop_columns = [\n",
    "    'disease_code',\n",
    "    'serology_result',          # result of serological test (positive, negative, inconclusive)\n",
    "    'ns1_result',               # result of NS1 antigen test\n",
    "    'viral_isolation_result',   # result of viral isolation test\n",
    "    'pcr_result',               # result of RT-PCR test\n",
    "    'serotype',                 # dengue serotype identified (DENV-1, 2, 3, or 4)\n",
    "    'histopathology',           # histopathology result\n",
    "    'immunohistochemistry',     # immunohistochemistry result\n",
    "    'hemorrhagic_manifest'\n",
    "]\n",
    "\n",
    "df = df.drop(columns=drop_columns, errors='ignore')  # ignore errors for columns that may not exist in all datasets\n",
    "df = df.drop(columns=lab_drop_columns, errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd35206",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Derivando features com as datas\n",
    "df['notification_date'] = pd.to_datetime(df['notification_date'], errors='coerce')\n",
    "df['symptom_onset_date'] = pd.to_datetime(df['symptom_onset_date'], errors='coerce')\n",
    "\n",
    "df['symptom_month'] = df['symptom_onset_date'].dt.month\n",
    "df['symptom_day'] = df['symptom_onset_date'].dt.day\n",
    "df['symptom_month_end'] = df['symptom_onset_date'].dt.is_month_end\n",
    "df['symptom_year_end'] = df['symptom_onset_date'].dt.is_year_end\n",
    "\n",
    "# Dias entre início dos sintomas e notificação (janela crítica da dengue: 3-6 dias)\n",
    "df['days_to_notification'] = (df['notification_date'] - df['symptom_onset_date']).dt.days\n",
    "df['days_to_notification'] = df['days_to_notification'].fillna(df['days_to_notification'].median())\n",
    "df['days_to_notification'] = df['days_to_notification'].clip(0, 90)\n",
    "\n",
    "# Derivando idade a partir da data de nascimento\n",
    "try:\n",
    "    df['birth_date'] = pd.to_datetime(df['birth_date'], errors='ignore')\n",
    "    df['birth_year'] = df['birth_date'].dt.year\n",
    "    df['age'] = df['birth_year'].apply(lambda x: 2025 - x if pd.notnull(x) else None)\n",
    "    df = df.drop(columns=['birth_date', 'birth_year', 'notification_date', 'symptom_onset_date'])\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error processing birth date: {e}\")\n",
    "    df['age'] = df['age'] - 4000\n",
    "    df = df.drop(columns=['notification_date', 'symptom_onset_date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9df4bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertendo colunas booleanas (1=Yes, 2=No) para 1/0\n",
    "bools = df.select_dtypes(include=['bool']).columns\n",
    "df[bools] = df[bools].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9lqxovd0uxs",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_columns = [\n",
    "    'sex',\n",
    "    'pregnancy_status',\n",
    "    'race',\n",
    "    'education_level',\n",
    "    'occupation_code',\n",
    "    'symptom_month',\n",
    "    'symptom_day',\n",
    "    'residence_state',\n",
    "    'symptom_epi_week'\n",
    "]\n",
    "\n",
    "# This keeps all indices non-negative, which is required by nn.Embedding.\n",
    "oe = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1)\n",
    "df[categorical_columns] = oe.fit_transform(df[categorical_columns]) + 1\n",
    "\n",
    "# Fill with 0, which is the reserved \"unknown\" token from the shift above.\n",
    "df[categorical_columns] = df[categorical_columns].fillna(0).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d7ibvfjyi6",
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_columns = [\n",
    "    # === SINTOMAS ===\n",
    "    'fever', 'myalgia', 'headache', 'rash', 'vomiting', 'nausea',\n",
    "    'back_pain', 'conjunctivitis', 'arthritis', 'joint_pain',\n",
    "    'petechiae', 'retro_orbital_pain',\n",
    "\n",
    "    # === COMORBIDADES ===\n",
    "    'diabetes', 'blood_disorder', 'liver_disease', 'kidney_disease',\n",
    "    'hypertension', 'peptic_ulcer', 'autoimmune_disease',\n",
    "\n",
    "    # === MANIFESTAÇÕES HEMORRÁGICAS ===\n",
    "    'nosebleed', 'gum_bleeding', 'metrorrhagia',\n",
    "    'petechiae_hemorrh', 'hematuria', 'other_bleeding',\n",
    "]\n",
    "\n",
    "# 1=Sim, 2=Não, 9=Ignorado → 1=Sim, 0=Não/Ignorado/NaN\n",
    "df[binary_columns] = df[binary_columns].replace({2: 0, 9: 0}).fillna(0).astype(int)\n",
    "\n",
    "# age: preencher NaN com mediana\n",
    "df['age'] = df['age'].fillna(df['age'].median())\n",
    "df['residence_health_region'] = df['residence_health_region'].fillna(df['residence_health_region'].median()).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "p0a144i4ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "symptom_cols = [\n",
    "    'fever', 'myalgia', 'headache', 'rash', 'vomiting', 'nausea', 'back_pain', 'conjunctivitis',\n",
    "    'arthritis', 'joint_pain', 'petechiae', 'retro_orbital_pain',\n",
    "]\n",
    "comorbidity_cols = ['diabetes', 'blood_disorder', 'liver_disease', 'kidney_disease', 'hypertension', 'peptic_ulcer', 'autoimmune_disease']\n",
    "hemorrhagic_cols = ['nosebleed', 'gum_bleeding', 'metrorrhagia', 'petechiae_hemorrh', 'hematuria', 'other_bleeding']\n",
    "\n",
    "df['symptom_count']     = df[symptom_cols].sum(axis=1)\n",
    "df['comorbidity_count'] = df[comorbidity_cols].sum(axis=1)\n",
    "df['hemorrhagic_count'] = df[hemorrhagic_cols].sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sxl9l5honv",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "\n",
    "# Todas as combinações par a par dos 12 sintomas clínicos auto-reportáveis\n",
    "symptom_columns = [\n",
    "    'fever', 'myalgia', 'headache', 'rash', 'vomiting', 'nausea', 'back_pain', 'conjunctivitis', \n",
    "    'arthritis', 'joint_pain', 'petechiae', 'retro_orbital_pain',\n",
    "]\n",
    "\n",
    "interaction_cols = {\n",
    "    f'{a}_and_{b}': (df[a] * df[b]).astype(int)\n",
    "    for a, b in combinations(symptom_columns, 2)\n",
    "}\n",
    "\n",
    "interaction_cols_3 = {\n",
    "    f'{a}_{b}_{c}': (df[a] * df[b] * df[c]).astype(int)\n",
    "    for a, b, c in combinations(symptom_columns, 3)\n",
    "}\n",
    "\n",
    "df = pd.concat([df, pd.DataFrame(interaction_cols, index=df.index)], axis=1)\n",
    "# df = pd.concat([df, pd.DataFrame(interaction_cols_3, index=df.index)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "l8pdtpmzrqi",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove colunas onde >95% dos valores são iguais (baixa variância)\n",
    "dominance_threshold = 0.99\n",
    "\n",
    "dominant_ratio = df.drop(columns=['final_classification']).apply(\n",
    "    lambda col: col.value_counts(normalize=True).iloc[0]\n",
    ")\n",
    "cols_to_drop_low_variance = dominant_ratio[dominant_ratio >= dominance_threshold].index.tolist()\n",
    "\n",
    "# Não dropar colunas categóricas — variância delas é esperada ser concentrada após encoding\n",
    "cols_to_drop_low_variance = [c for c in cols_to_drop_low_variance if c not in categorical_columns]\n",
    "\n",
    "df = df.drop(columns=cols_to_drop_low_variance)\n",
    "\n",
    "print(f'Colunas removidas (>{dominance_threshold*100:.0f}% mesmo valor): {len(cols_to_drop_low_variance)}')\n",
    "print(cols_to_drop_low_variance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5e1b6e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tratando a coluna de target\n",
    "if type_disease == 'dengue':\n",
    "    df = df[df['final_classification'].isin([5, 10, 11, 12])]\n",
    "    \n",
    "if type_disease == 'chikungunya':\n",
    "    df = df[df['final_classification'].isin([5, 13])]\n",
    "\n",
    "dengue_mapping = {\n",
    "    5 : 0,   # Discarded\n",
    "    10 : 1,  # Confirmed\n",
    "    11 : 1,  # Confirmed and alarming\n",
    "    12 : 1,  # Confirmed with complications\n",
    "}\n",
    "\n",
    "chik_mapping = {\n",
    "    5 : 0,   # Discarded\n",
    "    13 : 1,  # Confirmed Chikungunya\n",
    "}\n",
    "\n",
    "df['final_classification'] = df['final_classification'].map(dengue_mapping if type_disease == 'dengue' else chik_mapping).fillna(0).astype(int)\n",
    "df['final_classification'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db2ea96a",
   "metadata": {},
   "source": [
    "# Machine Learning Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a23b09a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_tensors = torch.tensor(df[categorical_columns].values, dtype=torch.long).to(device)\n",
    "numerical_tensors = torch.tensor(df.drop(columns=categorical_columns + ['final_classification']).values, dtype=torch.float).to(device)\n",
    "target_tensor = torch.tensor(df['final_classification'].values, dtype=torch.long).to(device)\n",
    "\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e2a8b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_cat = [df[col].max() + 1 for col in categorical_columns]\n",
    "embedding_sizes = [(size, min(50, (size // 2) + 1)) for size in unique_cat]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2feae18e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DengueTabularNN(nn.Module):\n",
    "    def __init__(self, numericals_shape, embedding_sizes, hidden_layers = [600, 300, 200, 100], probability_dropout = [0.05, 0.3]):\n",
    "        super().__init__()\n",
    "\n",
    "        # Tratamento dos embeddings\n",
    "        lista_embeddings = [nn.Embedding(size, new_size) for size, new_size in embedding_sizes]\n",
    "        self.embeddings = nn.ModuleList(lista_embeddings)\n",
    "        self.dropout_embeddings = nn.Dropout(p = probability_dropout[0])\n",
    "\n",
    "        # Normalização numéricas\n",
    "        self.normalization = nn.BatchNorm1d(numericals_shape)\n",
    "\n",
    "        # Soma para a entrada da primeira camada pra hidden layer\n",
    "        sum_columns = sum([embedding_sizes[i][1] for i in range(len(embedding_sizes))]) + numericals_shape\n",
    "\n",
    "        layers = []\n",
    "        current_entries = sum_columns\n",
    "        for layer_neurons in hidden_layers:\n",
    "            layers.append(nn.Linear(current_entries, layer_neurons))\n",
    "            layers.append(nn.LeakyReLU())\n",
    "            layers.append(nn.BatchNorm1d(layer_neurons))\n",
    "            layers.append(nn.Dropout(p = probability_dropout[1]))\n",
    "            current_entries = layer_neurons\n",
    "\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "        self.output_layer = nn.Linear(current_entries, 1)\n",
    "\n",
    "    def _engineering_embeddings(self, x_categorical):\n",
    "        embedded = []\n",
    "        for i, embedding in enumerate(self.embeddings):\n",
    "            embedded.append(embedding(x_categorical[:, i]))\n",
    "        return torch.cat(embedded, dim=1)\n",
    "    \n",
    "    def forward(self, x_categorical, x_numerical):\n",
    "        x_categorical = self._engineering_embeddings(x_categorical)\n",
    "        x_categorical = self.dropout_embeddings(x_categorical)\n",
    "        x_numerical = self.normalization(x_numerical)\n",
    "        x = torch.cat([x_categorical, x_numerical], dim=1)\n",
    "        x = self.layers(x)\n",
    "        return self.output_layer(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2b802bd",
   "metadata": {},
   "source": [
    "TensorDataset -> DataLoader -> Criterion -> Optimizer -> Scheduler -> Epochs Train and Validation -> SchedulerStep -> Early Stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "476934c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "x_train_cat, x_test_cat, x_train_num, x_test_num, y_train, y_test = train_test_split(categorical_tensors, numerical_tensors, target_tensor, test_size=0.1, shuffle=True, random_state=42)\n",
    "\n",
    "train_dataset = TensorDataset(x_train_cat, x_train_num, y_train)\n",
    "train_loader = DataLoader(train_dataset, batch_size=4196, shuffle=True)\n",
    "\n",
    "test_dataset = TensorDataset(x_test_cat, x_test_num, y_test)\n",
    "test_loader = DataLoader(test_dataset, batch_size=4196, shuffle=False)\n",
    "\n",
    "dengue_model = DengueTabularNN(embedding_sizes=embedding_sizes, hidden_layers=[1024, 512, 256, 128], numericals_shape=x_train_num.shape[1], probability_dropout=[0.1, 0.2]).to(device)\n",
    "pos_weight = (y_train == 0).sum().float() / (y_train == 1).sum().float()\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight.to(device))\n",
    "optimizer = torch.optim.AdamW(params=dengue_model.parameters(), lr = 2e-4, weight_decay=1e-4)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=3, factor=0.5, min_lr=1e-6)\n",
    "epochs = 150\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "patience = 6\n",
    "counter = 0\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    dengue_model.train()\n",
    "    epoch_train_loss = 0\n",
    "    for cat, num, target in train_loader:\n",
    "        cat, num, target = cat.to(device), num.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        pred = dengue_model(cat, num)\n",
    "        loss = criterion(pred, target.unsqueeze(1).float())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_train_loss += loss.item() * len(cat)\n",
    "\n",
    "    avg_train_loss = epoch_train_loss / len(train_dataset)\n",
    "    train_losses.append(avg_train_loss)\n",
    "\n",
    "    dengue_model.eval()\n",
    "    epoch_val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for cat, num, target in test_loader:\n",
    "            cat, num, target = cat.to(device), num.to(device), target.to(device)\n",
    "            pred = dengue_model(cat, num)\n",
    "            loss = criterion(pred, target.unsqueeze(1).float())\n",
    "            epoch_val_loss += loss.item() * len(cat)\n",
    "\n",
    "    avg_val_loss = epoch_val_loss / len(test_dataset)\n",
    "    val_losses.append(avg_val_loss)\n",
    "    scheduler.step(avg_val_loss)\n",
    "\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        counter = 0\n",
    "        torch.save(dengue_model.state_dict(), 'best_dengue_model.pth')\n",
    "    else:\n",
    "        counter += 1\n",
    "        if counter >= patience:\n",
    "            print(f'Early stopping at epoch {epoch}')\n",
    "            break\n",
    "\n",
    "    print(f'Epoch: {epoch:3d} | Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f} | LR: {scheduler._last_lr[0]:.6f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14ec9f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(train_losses, label='Train Loss')\n",
    "plt.plot(val_losses, label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('MSE Loss')\n",
    "plt.legend()\n",
    "plt.title('Training Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f29823",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "\n",
    "dengue_model = DengueTabularNN(embedding_sizes=embedding_sizes, hidden_layers=[1024, 512, 256, 128], numericals_shape=x_train_num.shape[1], probability_dropout=[0.1, 0.2]).to(device)\n",
    "dengue_model.load_state_dict(torch.load('best_dengue_model.pth', weights_only=True))\n",
    "dengue_model.eval()\n",
    "\n",
    "all_probs = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for X_cat_batch, X_num_batch, _ in test_loader:\n",
    "        y_pred_batch = dengue_model(X_cat_batch, X_num_batch)\n",
    "        all_probs.append(torch.sigmoid(y_pred_batch).squeeze().cpu())\n",
    "\n",
    "probabilities = torch.cat(all_probs)\n",
    "y_true = y_test.cpu()\n",
    "\n",
    "# Busca pelo melhor threshold\n",
    "print(f'{\"Threshold\":>10} | {\"Accuracy\":>10} | {\"Precision\":>10} | {\"Recall\":>10} | {\"F1\":>10}')\n",
    "print('-' * 60)\n",
    "for t in [0.3, 0.35, 0.4, 0.45, 0.5, 0.55, 0.6]:\n",
    "    preds = (probabilities > t).long()\n",
    "    print(f'{t:>10.2f} | {(preds == y_true).float().mean().item():>10.4f} | {precision_score(y_true, preds):>10.4f} | {recall_score(y_true, preds):>10.4f} | {f1_score(y_true, preds):>10.4f}')\n",
    "\n",
    "predicted_classes = (probabilities > 0.3).long()\n",
    "\n",
    "test = pd.DataFrame({\n",
    "    'Actual': y_true.numpy(),\n",
    "    'Prob': probabilities.numpy(),\n",
    "    'Predicted': predicted_classes.numpy(),\n",
    "    'Correct': (y_true == predicted_classes).numpy()\n",
    "})\n",
    "display(test.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "652dcedb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.inspection import permutation_importance\n",
    "import numpy as np\n",
    "\n",
    "# Subsample first, then concatenate\n",
    "idx = np.random.choice(x_test_cat.shape[0], size=2000, replace=False)\n",
    "\n",
    "X_test = np.concatenate([\n",
    "    x_test_cat[idx].cpu().numpy(),\n",
    "    x_test_num[idx].cpu().numpy().astype(np.float32)\n",
    "], axis=1)\n",
    "y_test_np = y_test[idx].cpu().numpy().astype(int).flatten()\n",
    "\n",
    "n_cat = x_test_cat.shape[1]  # define outside wrapper to avoid closure issues\n",
    "\n",
    "class SklearnWrapper:\n",
    "    def fit(self, X, y):\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        cat = torch.tensor(X[:, :n_cat], dtype=torch.long).to(device)\n",
    "        num = torch.tensor(X[:, n_cat:], dtype=torch.float32).to(device)\n",
    "        with torch.no_grad():\n",
    "            out = dengue_model(cat, num)\n",
    "        return (out.cpu().numpy() > 0.5).astype(int).flatten()\n",
    "\n",
    "    def score(self, X, y):\n",
    "        from sklearn.metrics import accuracy_score\n",
    "        return accuracy_score(y, self.predict(X))\n",
    "\n",
    "wrapper = SklearnWrapper()\n",
    "result = permutation_importance(wrapper, X_test, y_test_np, n_repeats=100, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf99e42e",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_n = 20\n",
    "\n",
    "all_feature_names = categorical_columns + list(df.drop(columns=categorical_columns + ['final_classification']).columns)\n",
    "sorted_idx = result.importances_mean.argsort()[::-1][:top_n]\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.bar(range(top_n), result.importances_mean[sorted_idx], yerr=result.importances_std[sorted_idx])\n",
    "plt.xticks(range(top_n), [all_feature_names[i] for i in sorted_idx], rotation=90)\n",
    "plt.title(f'Permutation Feature Importance - Top {top_n}')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vnlvtbzh32",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "X_train_lgbm = pd.concat([\n",
    "    pd.DataFrame(x_train_cat.cpu().numpy(), columns=categorical_columns),\n",
    "    pd.DataFrame(x_train_num.cpu().numpy(), columns=df.drop(columns=categorical_columns + ['final_classification']).columns)\n",
    "], axis=1)\n",
    "\n",
    "X_test_lgbm = pd.concat([\n",
    "    pd.DataFrame(x_test_cat.cpu().numpy(), columns=categorical_columns),\n",
    "    pd.DataFrame(x_test_num.cpu().numpy(), columns=df.drop(columns=categorical_columns + ['final_classification']).columns)\n",
    "], axis=1)\n",
    "\n",
    "lgbm_model = LGBMClassifier(\n",
    "    n_estimators=2000,\n",
    "    learning_rate=0.03,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    device='gpu',\n",
    ")\n",
    "\n",
    "lgbm_model.fit(X_train_lgbm, y_train.cpu().numpy())\n",
    "\n",
    "lgbm_accuracy = (lgbm_model.predict(X_test_lgbm) == y_test.cpu().numpy()).mean()\n",
    "print(f'LightGBM Accuracy: {lgbm_accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "qm5dl79jd0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = pd.Series(lgbm_model.feature_importances_, index=X_train_lgbm.columns)\n",
    "importances = importances.sort_values(ascending=False)\n",
    "    \n",
    "plt.figure(figsize=(12, 8))\n",
    "importances.head(30).plot(kind='bar')\n",
    "plt.title('LightGBM - Top 30 Feature Importances')\n",
    "plt.ylabel('Importance (F-score)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(importances.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10f4af3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy, Precision, Recall e F1 para o modelo XGBoost based on thresholds list\n",
    "\n",
    "for t in [0.1, 0.3, 0.35, 0.4, 0.45, 0.5, 0.55, 0.6]:\n",
    "    xgb_probs = lgbm_model.predict_proba(X_test_lgbm)[:, 1]\n",
    "    xgb_preds = (xgb_probs > t).astype(int)\n",
    "    print(f'Threshold: {t:.2f} | Accuracy: {(xgb_preds == y_test.cpu().numpy()).mean():.4f} | Precision: {precision_score(y_test.cpu().numpy(), xgb_preds):.4f} | Recall: {recall_score(y_test.cpu().numpy(), xgb_preds):.4f} | F1: {f1_score(y_test.cpu().numpy(), xgb_preds):.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
