{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "94443428",
   "metadata": {},
   "source": [
    "## 1. Setup & Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4b46dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "from itertools import combinations\n",
    "\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "print(f'Python: {sys.executable}')\n",
    "print(f'PyTorch: {torch.__version__}')\n",
    "print(f'CUDA built with: {torch.version.cuda}')\n",
    "print(f'CUDA available: {torch.cuda.is_available()}')\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bu4bunujh8t",
   "metadata": {},
   "source": [
    "# Dengue Classification — Neural Network\n",
    "\n",
    "Binary classification of dengue cases as **confirmed** (1) or **discarded** (0), using clinical and demographic data from Brazil's SINAN surveillance system (2017–2019).\n",
    "\n",
    "**Dataset:** ~3.3M records across 3 years  \n",
    "**Target:** `final_classification` → 0 = discarded, 1 = confirmed (classes 10, 11, 12)  \n",
    "**Features used:** self-reportable symptoms, demographics, comorbidities, hemorrhagic signs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b61eb62",
   "metadata": {},
   "source": [
    "## 2. Data Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0e7a565",
   "metadata": {},
   "source": [
    "### 2.1 Load CSVs and merge years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6444c158",
   "metadata": {},
   "outputs": [],
   "source": [
    "type_disease = 'chikungunya'  # Change to 'chikungunya' for Chikungunya dataset\n",
    "\n",
    "if type_disease == 'dengue':\n",
    "    df1 = pd.read_csv(\"C:\\\\Users\\\\angej\\\\Documents\\\\2_Programação\\\\health_index_project\\\\data\\\\DENGBR17.csv\", low_memory=False)\n",
    "    df4 = pd.read_csv(\"C:\\\\Users\\\\angej\\\\Documents\\\\2_Programação\\\\health_index_project\\\\data\\\\DENGBR18.csv\", low_memory=False)\n",
    "    df2 = pd.read_csv(\"C:\\\\Users\\\\angej\\\\Documents\\\\2_Programação\\\\health_index_project\\\\data\\\\DENGBR19.csv\", low_memory=False)\n",
    "\n",
    "elif type_disease == 'chikungunya':\n",
    "    df1 = pd.read_csv(\"C:\\\\Users\\\\angej\\\\Documents\\\\2_Programação\\\\health_index_project\\\\data\\\\CHIKBR17.csv\", low_memory=False)\n",
    "    df4 = pd.read_csv(\"C:\\\\Users\\\\angej\\\\Documents\\\\2_Programação\\\\health_index_project\\\\data\\\\CHIKBR18.csv\", low_memory=False)\n",
    "    df2 = pd.read_csv(\"C:\\\\Users\\\\angej\\\\Documents\\\\2_Programação\\\\health_index_project\\\\data\\\\CHIKBR19.csv\", low_memory=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc073407",
   "metadata": {},
   "source": [
    "### 2.2 Downcast dtypes to reduce memory usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0330d0bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([df1, df2, df4], ignore_index=True)\n",
    "\n",
    "# Downcast integer columns to smallest fitting type\n",
    "int_cols = df.select_dtypes(\"int64\").columns\n",
    "df[int_cols] = df[int_cols].apply(pd.to_numeric, downcast=\"integer\")\n",
    "\n",
    "# Downcast float columns\n",
    "float_cols = df.select_dtypes(\"float64\").columns\n",
    "df[float_cols] = df[float_cols].apply(pd.to_numeric, downcast=\"float\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff742761",
   "metadata": {},
   "source": [
    "## 3. Feature Engineering\n",
    "\n",
    "### 3.1 Rename columns to English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fc72b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.rename(columns={\n",
    "    # === NOTIFICATION INFO ===\n",
    "    'TP_NOT': 'notification_type',          # Type of notification (individual, aggregate, etc.)\n",
    "    'ID_AGRAVO': 'disease_code',            # ICD/SINAN code identifying the disease\n",
    "    'DT_NOTIFIC': 'notification_date',      # Date the case was reported\n",
    "    'SEM_NOT': 'notification_epi_week',     # Epidemiological week of the notification\n",
    "    'NU_ANO': 'notification_year',          # Year the case was reported\n",
    "    'SG_UF_NOT': 'notif_state',            # State (UF) where the case was notified\n",
    "    'ID_MUNICIP': 'notif_municipality',     # Municipality where the case was notified\n",
    "    'ID_REGIONA': 'notif_health_region',    # Health region where the case was notified\n",
    "    'ID_UNIDADE': 'health_facility',        # Health facility that filed the notification\n",
    "    'DT_SIN_PRI': 'symptom_onset_date',     # Date patient first showed symptoms\n",
    "    'SEM_PRI': 'symptom_epi_week',          # Epidemiological week of first symptoms\n",
    "\n",
    "    # === PATIENT DEMOGRAPHICS ===\n",
    "    'ANO_NASC': 'birth_year',              # Patient's year of birth\n",
    "    'NU_IDADE_N': 'age',                   # Patient's age (encoded with unit prefix: days/months/years)\n",
    "    'CS_SEXO': 'sex',                      # Patient's sex (M=male, F=female, I=ignored)\n",
    "    'CS_GESTANT': 'pregnancy_status',       # Pregnancy trimester (1st, 2nd, 3rd) or N/A\n",
    "    'CS_RACA': 'race',                     # Patient's race/ethnicity\n",
    "    'CS_ESCOL_N': 'education_level',        # Patient's education level\n",
    "    'ID_OCUPA_N': 'occupation_code',        # Patient's occupation (CBO code)\n",
    "\n",
    "    # === PATIENT RESIDENCE ===\n",
    "    'SG_UF': 'residence_state',            # State where the patient lives\n",
    "    'ID_MN_RESI': 'residence_municipality', # Municipality where the patient lives\n",
    "    'ID_RG_RESI': 'residence_health_region',# Health region where the patient lives\n",
    "    'ID_PAIS': 'residence_country',         # Country where the patient lives\n",
    "\n",
    "    # === SYMPTOMS (1=Yes, 2=No, 9=Unknown) ===\n",
    "    'FEBRE': 'fever',                       # Fever\n",
    "    'MIALGIA': 'myalgia',                  # Muscle pain\n",
    "    'CEFALEIA': 'headache',                # Headache\n",
    "    'EXANTEMA': 'rash',                    # Skin rash\n",
    "    'VOMITO': 'vomiting',                  # Vomiting\n",
    "    'NAUSEA': 'nausea',                    # Nausea\n",
    "    'DOR_COSTAS': 'back_pain',             # Back pain\n",
    "    'CONJUNTVIT': 'conjunctivitis',        # Conjunctivitis (eye inflammation)\n",
    "    'ARTRITE': 'arthritis',                # Joint inflammation\n",
    "    'ARTRALGIA': 'joint_pain',             # Joint pain\n",
    "    'PETEQUIA_N': 'petechiae',             # Small red/purple spots on skin (bleeding under skin)\n",
    "    'LEUCOPENIA': 'leucopenia',            # Low white blood cell count\n",
    "    'LACO': 'tourniquet_test',             # Tourniquet test (prova do laço) for capillary fragility\n",
    "    'DOR_RETRO': 'retro_orbital_pain',     # Pain behind the eyes\n",
    "\n",
    "    # === COMORBIDITIES (1=Yes, 2=No, 9=Unknown) ===\n",
    "    'DIABETES': 'diabetes',                 # Has diabetes\n",
    "    'HEMATOLOG': 'blood_disorder',          # Has blood/hematological disease\n",
    "    'HEPATOPAT': 'liver_disease',           # Has liver disease\n",
    "    'RENAL': 'kidney_disease',              # Has kidney disease\n",
    "    'HIPERTENSA': 'hypertension',           # Has hypertension\n",
    "    'ACIDO_PEPT': 'peptic_ulcer',           # Has peptic acid disease / ulcer\n",
    "    'AUTO_IMUNE': 'autoimmune_disease',     # Has autoimmune disease\n",
    "\n",
    "    # === CHIKUNGUNYA LAB TESTS ===\n",
    "    'DT_CHIK_S1': 'chik_test1_date',       # Date of Chikungunya serological test 1\n",
    "    'DT_CHIK_S2': 'chik_test2_date',       # Date of Chikungunya serological test 2\n",
    "    'RES_CHIKS1': 'chik_test1_result',      # Result of Chikungunya test 1\n",
    "    'RES_CHIKS2': 'chik_test2_result',      # Result of Chikungunya test 2\n",
    "    'DT_PRNT': 'prnt_date',                # Date of PRNT test (plaque reduction neutralization)\n",
    "    'RESUL_PRNT': 'prnt_result',            # Result of PRNT test\n",
    "\n",
    "    # === DENGUE LAB TESTS ===\n",
    "    'DT_SORO': 'serology_date',            # Date of serological test (IgM)\n",
    "    'RESUL_SORO': 'serology_result',        # Result of serology (positive, negative, inconclusive)\n",
    "    'DT_NS1': 'ns1_test_date',             # Date of NS1 antigen test\n",
    "    'RESUL_NS1': 'ns1_result',             # Result of NS1 test\n",
    "    'DT_VIRAL': 'viral_isolation_date',     # Date of viral isolation test\n",
    "    'RESUL_VI_N': 'viral_isolation_result', # Result of viral isolation\n",
    "    'DT_PCR': 'pcr_date',                  # Date of RT-PCR test\n",
    "    'RESUL_PCR_': 'pcr_result',            # Result of RT-PCR test\n",
    "    'SOROTIPO': 'serotype',                # Dengue serotype identified (DENV-1, 2, 3, or 4)\n",
    "    'HISTOPA_N': 'histopathology',         # Histopathology result\n",
    "    'IMUNOH_N': 'immunohistochemistry',    # Immunohistochemistry result\n",
    "\n",
    "    # === HOSPITALIZATION ===\n",
    "    'HOSPITALIZ': 'hospitalized',           # Whether patient was hospitalized (1=Yes, 2=No)\n",
    "    'DT_INTERNA': 'hospitalization_date',   # Date of hospitalization\n",
    "    'UF': 'hospital_state',                # State of the hospital\n",
    "    'MUNICIPIO': 'hospital_municipality',   # Municipality of the hospital\n",
    "\n",
    "    # === INFECTION ORIGIN ===\n",
    "    'TPAUTOCTO': 'autochthonous_case',      # Whether infection was local or imported\n",
    "    'COUFINF': 'infection_state',           # State where infection likely occurred\n",
    "    'COPAISINF': 'infection_country',       # Country where infection likely occurred\n",
    "    'COMUNINF': 'infection_municipality',   # Municipality where infection likely occurred\n",
    "\n",
    "    # === CLASSIFICATION & OUTCOME ===\n",
    "    'CLASSI_FIN': 'final_classification',   # Final diagnosis (confirmed, discarded, inconclusive)\n",
    "    'CRITERIO': 'confirmation_criteria',    # How it was confirmed (lab, clinical, epidemiological)\n",
    "    'DOENCA_TRA': 'work_related',           # Whether the disease is work-related\n",
    "    'CLINC_CHIK': 'chik_clinical_form',     # Clinical form of Chikungunya (acute, subacute, chronic)\n",
    "    'EVOLUCAO': 'case_outcome',             # Patient outcome (cured, died, etc.)\n",
    "    'DT_OBITO': 'death_date',              # Date of death (if applicable)\n",
    "    'DT_ENCERRA': 'case_closure_date',      # Date the case was officially closed\n",
    "\n",
    "    # === ALARM SIGNS (dengue warning signs, 1=Yes, 2=No) ===\n",
    "    'ALRM_HIPOT': 'alarm_hypotension',     # Postural hypotension (drop in blood pressure)\n",
    "    'ALRM_PLAQ': 'alarm_low_platelets',    # Platelet count drop\n",
    "    'ALRM_VOM': 'alarm_persistent_vomit',  # Persistent vomiting\n",
    "    'ALRM_SANG': 'alarm_bleeding',         # Bleeding from mucous membranes\n",
    "    'ALRM_HEMAT': 'alarm_hematocrit_rise', # Rising hematocrit\n",
    "    'ALRM_ABDOM': 'alarm_abdominal_pain',  # Intense abdominal pain\n",
    "    'ALRM_LETAR': 'alarm_lethargy',        # Lethargy / irritability\n",
    "    'ALRM_HEPAT': 'alarm_liver_enlarged',  # Enlarged liver (hepatomegaly)\n",
    "    'ALRM_LIQ': 'alarm_fluid_accumul',     # Fluid accumulation (pleural effusion, ascites)\n",
    "    'DT_ALRM': 'alarm_signs_date',         # Date alarm signs were observed\n",
    "\n",
    "    # === SEVERITY SIGNS (severe dengue, 1=Yes, 2=No) ===\n",
    "    'GRAV_PULSO': 'severe_weak_pulse',      # Weak or absent pulse\n",
    "    'GRAV_CONV': 'severe_convulsions',      # Convulsions\n",
    "    'GRAV_ENCH': 'severe_cap_refill',       # Slow capillary refill (>2 sec)\n",
    "    'GRAV_INSUF': 'severe_resp_distress',   # Respiratory distress\n",
    "    'GRAV_TAQUI': 'severe_tachycardia',     # Tachycardia (fast heart rate)\n",
    "    'GRAV_EXTRE': 'severe_cold_extremities',# Cold extremities / cyanosis\n",
    "    'GRAV_HIPOT': 'severe_hypotension',     # Hypotension / shock\n",
    "    'GRAV_HEMAT': 'severe_hematemesis',     # Vomiting blood\n",
    "    'GRAV_MELEN': 'severe_melena',          # Black tarry stool (GI bleeding)\n",
    "    'GRAV_METRO': 'severe_metrorrhagia',    # Abnormal uterine bleeding\n",
    "    'GRAV_SANG': 'severe_bleeding',         # Severe bleeding\n",
    "    'GRAV_AST': 'severe_ast_elevated',      # AST/ALT > 1000 (liver enzymes)\n",
    "    'GRAV_MIOC': 'severe_myocarditis',      # Myocarditis (heart inflammation)\n",
    "    'GRAV_CONSC': 'severe_altered_consc',   # Altered consciousness\n",
    "    'GRAV_ORGAO': 'severe_organ_damage',    # Other organ involvement\n",
    "    'DT_GRAV': 'severity_signs_date',       # Date severity signs were observed\n",
    "\n",
    "    # === HEMORRHAGIC MANIFESTATIONS ===\n",
    "    'MANI_HEMOR': 'hemorrhagic_manifest',   # Had hemorrhagic manifestations (1=Yes, 2=No)\n",
    "    'EPISTAXE': 'nosebleed',                # Epistaxis (nosebleed)\n",
    "    'GENGIVO': 'gum_bleeding',              # Gingival bleeding (gums)\n",
    "    'METRO': 'metrorrhagia',                # Abnormal uterine bleeding\n",
    "    'PETEQUIAS': 'petechiae_hemorrh',       # Petechiae (hemorrhagic context)\n",
    "    'HEMATURA': 'hematuria',                # Blood in urine\n",
    "    'SANGRAM': 'other_bleeding',            # Other bleeding\n",
    "    'LACO_N': 'tourniquet_test_hemorrh',    # Tourniquet test (hemorrhagic context)\n",
    "    'PLASMATICO': 'plasma_leakage',         # Evidence of plasma leakage\n",
    "    'EVIDENCIA': 'hemorrhagic_evidence',    # Evidence of hemorrhagic manifestation\n",
    "    'PLAQ_MENOR': 'platelets_below_100k',   # Platelet count < 100,000\n",
    "    'CON_FHD': 'dengue_hemorrhagic_fever',  # Confirmed Dengue Hemorrhagic Fever (DHF)\n",
    "    'COMPLICA': 'complications',            # Complications present\n",
    "\n",
    "    # === ADMINISTRATIVE / SYSTEM ===\n",
    "    'DT_INVEST': 'investigation_date',      # Date the case was investigated\n",
    "    'DT_DIGITA': 'data_entry_date',         # Date the record was entered into the system\n",
    "    'TP_SISTEMA': 'system_type',            # Type of information system used\n",
    "    'NDUPLIC_N': 'duplicate_flag',          # Whether this record is a duplicate\n",
    "    'CS_FLXRET': 'return_flow_flag',        # Case flow return flag (inter-state data sharing)\n",
    "    'FLXRECEBI': 'flow_received',           # Flow received flag (inter-state data sharing)\n",
    "    'MIGRADO_W': 'migrated_from_windows',   # Record migrated from old Windows SINAN system\n",
    "    'DT_NASC' : 'birth_date'                   # Patient's date of birth\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6076a8a2",
   "metadata": {},
   "source": [
    "### 3.2 Drop columns\n",
    "\n",
    "Drops are split into two groups:\n",
    "- **`drop_columns`** — administrative fields, post-classification data, label-leaking features (alarm/severity signs), and fields unavailable at diagnosis time (hospitalization, lab test dates)\n",
    "- **`lab_drop_columns`** — lab results and confirmation evidence not available during clinical triage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1982775",
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_columns = [\n",
    "    # === ADMINISTRATIVE (no predictive value) ===\n",
    "    'investigation_date',       # filled during/after investigation\n",
    "    'duplicate_flag',           # system control field\n",
    "    'return_flow_flag',         # system control field\n",
    "    'flow_received',            # system control field\n",
    "    'system_type',              # system control field\n",
    "    'notification_type',        # administrative notification type\n",
    "    'notification_epi_week',    # redundant with notification_month/day derived features\n",
    "\n",
    "    # === POST-CLASSIFICATION (filled after or because of final_classification) ===\n",
    "    'confirmation_criteria',    # directly tied to classification (lab, clinical, epidemiological)\n",
    "    'case_closure_date',        # required when classification is filled\n",
    "    'case_outcome',             # outcome recorded after classification (cura, obito, etc.)\n",
    "    'death_date',               # post-outcome\n",
    "    'work_related',             # enabled only if classification=1, cleared if classification=2\n",
    "    'chik_clinical_form',       # required only if classification=13 (Chikungunya)\n",
    "\n",
    "    # === INFECTION ORIGIN (filled only when classification=confirmed, cleared on discard) ===\n",
    "    'autochthonous_case',       # required only if classification=1\n",
    "    'infection_state',          # required only if classification=1\n",
    "    'infection_country',        # required only if classification=1\n",
    "    'infection_municipality',   # required only if classification=1\n",
    "\n",
    "    # === ALARM SIGNS (filled only when classification=11 or 12, leaks the label directly) ===\n",
    "    'alarm_hypotension',\n",
    "    'alarm_low_platelets',\n",
    "    'alarm_persistent_vomit',\n",
    "    'alarm_bleeding',\n",
    "    'alarm_hematocrit_rise',\n",
    "    'alarm_abdominal_pain',\n",
    "    'alarm_lethargy',\n",
    "    'alarm_liver_enlarged',\n",
    "    'alarm_fluid_accumul',\n",
    "    'alarm_signs_date',\n",
    "\n",
    "    # === SEVERITY SIGNS (filled only when classification=12, leaks the label directly) ===\n",
    "    'severe_weak_pulse',\n",
    "    'severe_convulsions',\n",
    "    'severe_cap_refill',\n",
    "    'severe_resp_distress',\n",
    "    'severe_tachycardia',\n",
    "    'severe_cold_extremities',\n",
    "    'severe_hypotension',\n",
    "    'severe_hematemesis',\n",
    "    'severe_melena',\n",
    "    'severe_metrorrhagia',\n",
    "    'severe_bleeding',\n",
    "    'severe_ast_elevated',\n",
    "    'severe_myocarditis',\n",
    "    'severe_altered_consc',\n",
    "    'severe_organ_damage',\n",
    "    'severity_signs_date',\n",
    "\n",
    "    # === DHF / COMPLICATIONS (old classification system, directly informs final_classification) ===\n",
    "    'dengue_hemorrhagic_fever', # confirmed DHF = classification decision\n",
    "    'complications',            # dengue with complications = classification decision\n",
    "\n",
    "    # === CHIKUNGUNYA TESTS (disabled for dengue cases per dictionary, ~97.7% NaN) ===\n",
    "    'chik_test1_date',          # enabled only for Chikungunya, dataset is 100% Dengue (A90)\n",
    "    'chik_test2_date',\n",
    "    'prnt_date',\n",
    "    'chik_test1_result',\n",
    "    'chik_test2_result',\n",
    "    'prnt_result',\n",
    "\n",
    "    # === HOSPITALIZATION (post-assessment decision, not available at diagnosis time) ===\n",
    "    'hospitalized',             # decision made after clinical evaluation\n",
    "    'hospitalization_date',     # only filled if hospitalized\n",
    "    'hospital_state',           # only filled if hospitalized\n",
    "    'hospital_municipality',    # only filled if hospitalized\n",
    "\n",
    "    # === DATE FIELDS (not useful as raw values for ANN, keeping only notification_date, symptom_onset_date, birth_year) ===\n",
    "    'notification_year',        # redundant with notification_date\n",
    "    'serology_date',            # lab test date, not useful as raw value\n",
    "    'ns1_test_date',            # lab test date\n",
    "    'viral_isolation_date',     # lab test date\n",
    "    'pcr_date',                 # lab test date\n",
    "\n",
    "    # === NOT SELF-REPORTABLE (require clinical procedure or lab exam) ===\n",
    "    'leucopenia',               # blood test\n",
    "    'tourniquet_test',          # clinical procedure (prova do laço)\n",
    "    'tourniquet_test_hemorrh',  # clinical procedure (hemorrhagic context)\n",
    "    'plasma_leakage',           # clinical evaluation\n",
    "    'platelets_below_100k',     # blood test\n",
    "    'hemorrhagic_evidence',     # clinical evaluation\n",
    "\n",
    "    # === GEOGRAPHICAL (not available/useful in a self-reported questionnaire) ===\n",
    "    'notif_state',\n",
    "    'notif_municipality',\n",
    "    'notif_health_region',\n",
    "    'health_facility',\n",
    "    'residence_municipality',\n",
    "    'residence_country',\n",
    "]\n",
    "\n",
    "lab_drop_columns = [\n",
    "    'disease_code',\n",
    "    'serology_result',          # result of serological test (positive, negative, inconclusive)\n",
    "    'ns1_result',               # result of NS1 antigen test\n",
    "    'viral_isolation_result',   # result of viral isolation test\n",
    "    'pcr_result',               # result of RT-PCR test\n",
    "    'serotype',                 # dengue serotype identified (DENV-1, 2, 3, or 4)\n",
    "    'histopathology',           # histopathology result\n",
    "    'immunohistochemistry',     # immunohistochemistry result\n",
    "    'hemorrhagic_manifest'\n",
    "]\n",
    "\n",
    "df = df.drop(columns=drop_columns, errors='ignore')  # ignore errors for columns that may not exist in all datasets\n",
    "df = df.drop(columns=lab_drop_columns, errors='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0f34b79",
   "metadata": {},
   "source": [
    "### 3.3 Date-derived features\n",
    "\n",
    "- `symptom_month`, `symptom_day` — seasonality signals\n",
    "- `symptom_month_end`, `symptom_year_end` — boundary flags\n",
    "- `days_to_notification` — delay between symptom onset and reporting (clipped to 0–90 days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd35206",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Derivando features com as datas\n",
    "df['notification_date'] = pd.to_datetime(df['notification_date'], errors='coerce')\n",
    "df['symptom_onset_date'] = pd.to_datetime(df['symptom_onset_date'], errors='coerce')\n",
    "\n",
    "df['symptom_month'] = df['symptom_onset_date'].dt.month\n",
    "df['symptom_day'] = df['symptom_onset_date'].dt.day\n",
    "df['symptom_month_end'] = df['symptom_onset_date'].dt.is_month_end\n",
    "df['symptom_year_end'] = df['symptom_onset_date'].dt.is_year_end\n",
    "\n",
    "# Dias entre início dos sintomas e notificação (janela crítica da dengue: 3-6 dias)\n",
    "df['days_to_notification'] = (df['notification_date'] - df['symptom_onset_date']).dt.days\n",
    "df['days_to_notification'] = df['days_to_notification'].fillna(df['days_to_notification'].median())\n",
    "df['days_to_notification'] = df['days_to_notification'].clip(0, 90)\n",
    "\n",
    "# Derivando idade a partir da data de nascimento\n",
    "if type_disease == 'dengue':\n",
    "    df['birth_date'] = pd.to_datetime(df['birth_date'])\n",
    "    df['birth_year'] = df['birth_date'].dt.year\n",
    "    df['age'] = df['birth_year'].apply(lambda x: 2025 - x if pd.notnull(x) else None)\n",
    "    df = df.drop(columns=['birth_date', 'birth_year', 'notification_date', 'symptom_onset_date'])\n",
    "\n",
    "elif type_disease == 'chikungunya':\n",
    "    df['age'] = df['age'] - 4000\n",
    "    df = df.drop(columns=['notification_date', 'symptom_onset_date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9df4bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertendo colunas booleanas (1=Yes, 2=No) para 1/0\n",
    "bools = df.select_dtypes(include=['bool']).columns\n",
    "df[bools] = df[bools].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25e38a1a",
   "metadata": {},
   "source": [
    "### 3.4 Encode categorical columns\n",
    "\n",
    "Uses `OrdinalEncoder` with `+1` shift so that index `0` is reserved as the \"unknown\" token for `nn.Embedding`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9lqxovd0uxs",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_columns = [\n",
    "    'sex',\n",
    "    'pregnancy_status',\n",
    "    'race',\n",
    "    'education_level',\n",
    "    'occupation_code',\n",
    "    'symptom_month',\n",
    "    'symptom_day',\n",
    "    'residence_state',\n",
    "    'symptom_epi_week'\n",
    "]\n",
    "\n",
    "# This keeps all indices non-negative, which is required by nn.Embedding.\n",
    "oe = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1)\n",
    "df[categorical_columns] = oe.fit_transform(df[categorical_columns]) + 1\n",
    "\n",
    "# Fill with 0, which is the reserved \"unknown\" token from the shift above.\n",
    "df[categorical_columns] = df[categorical_columns].fillna(0).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfefc4c9",
   "metadata": {},
   "source": [
    "### 3.5 Binarize symptom and comorbidity columns\n",
    "\n",
    "Original encoding: `1=Yes, 2=No, 9=Unknown` → mapped to `1=Yes, 0=No/Unknown/NaN`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d7ibvfjyi6",
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_columns = [\n",
    "    # === SINTOMAS ===\n",
    "    'fever', 'myalgia', 'headache', 'rash', 'vomiting', 'nausea',\n",
    "    'back_pain', 'conjunctivitis', 'arthritis', 'joint_pain',\n",
    "    'petechiae', 'retro_orbital_pain',\n",
    "\n",
    "    # === COMORBIDADES ===\n",
    "    'diabetes', 'blood_disorder', 'liver_disease', 'kidney_disease',\n",
    "    'hypertension', 'peptic_ulcer', 'autoimmune_disease',\n",
    "\n",
    "    # === MANIFESTAÇÕES HEMORRÁGICAS ===\n",
    "    'nosebleed', 'gum_bleeding', 'metrorrhagia',\n",
    "    'petechiae_hemorrh', 'hematuria', 'other_bleeding',\n",
    "]\n",
    "\n",
    "# 1=Sim, 2=Não, 9=Ignorado → 1=Sim, 0=Não/Ignorado/NaN\n",
    "df[binary_columns] = df[binary_columns].replace({2: 0, 9: 0}).fillna(0).astype(int)\n",
    "\n",
    "# age: preencher NaN com mediana\n",
    "df['age'] = df['age'].fillna(df['age'].median())\n",
    "df['residence_health_region'] = df['residence_health_region'].fillna(df['residence_health_region'].median()).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e02f5e90",
   "metadata": {},
   "source": [
    "### 3.6 Aggregate count features\n",
    "\n",
    "Summarizes symptom burden, comorbidity burden, and hemorrhagic signs into three scalar counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "p0a144i4ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "symptom_cols = [\n",
    "    'fever', 'myalgia', 'headache', 'rash', 'vomiting', 'nausea', 'back_pain', 'conjunctivitis',\n",
    "    'arthritis', 'joint_pain', 'petechiae', 'retro_orbital_pain',\n",
    "]\n",
    "comorbidity_cols = ['diabetes', 'blood_disorder', 'liver_disease', 'kidney_disease', 'hypertension', 'peptic_ulcer', 'autoimmune_disease']\n",
    "hemorrhagic_cols = ['nosebleed', 'gum_bleeding', 'metrorrhagia', 'petechiae_hemorrh', 'hematuria', 'other_bleeding']\n",
    "\n",
    "df['symptom_count']     = df[symptom_cols].sum(axis=1)\n",
    "df['comorbidity_count'] = df[comorbidity_cols].sum(axis=1)\n",
    "df['hemorrhagic_count'] = df[hemorrhagic_cols].sum(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d3cecbc",
   "metadata": {},
   "source": [
    "### 3.7 Pairwise symptom interactions\n",
    "\n",
    "Creates binary interaction features for all C(12, 2) = 66 symptom pairs.  \n",
    "Triplet interactions (C(12, 3) = 220 features) are available but disabled by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sxl9l5honv",
   "metadata": {},
   "outputs": [],
   "source": [
    "symptom_columns = [\n",
    "    'fever', 'myalgia', 'headache', 'rash', 'vomiting', 'nausea', 'back_pain', 'conjunctivitis', \n",
    "    'arthritis', 'joint_pain', 'petechiae', 'retro_orbital_pain',\n",
    "]\n",
    "\n",
    "interaction_cols = {\n",
    "    f'{a}_and_{b}': (df[a] * df[b]).astype(int)\n",
    "    for a, b in combinations(symptom_columns, 2)\n",
    "}\n",
    "\n",
    "interaction_cols_3 = {\n",
    "    f'{a}_{b}_{c}': (df[a] * df[b] * df[c]).astype(int)\n",
    "    for a, b, c in combinations(symptom_columns, 3)\n",
    "}\n",
    "\n",
    "df = pd.concat([df, pd.DataFrame(interaction_cols, index=df.index)], axis=1)\n",
    "# df = pd.concat([df, pd.DataFrame(interaction_cols_3, index=df.index)], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f0e26ea",
   "metadata": {},
   "source": [
    "### 3.8 Drop near-constant columns\n",
    "\n",
    "Removes any column (excluding categorical) where a single value dominates ≥99% of rows — these carry almost no information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "l8pdtpmzrqi",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove colunas onde >95% dos valores são iguais (baixa variância)\n",
    "dominance_threshold = 0.99\n",
    "\n",
    "dominant_ratio = df.drop(columns=['final_classification']).apply(\n",
    "    lambda col: col.value_counts(normalize=True).iloc[0]\n",
    ")\n",
    "cols_to_drop_low_variance = dominant_ratio[dominant_ratio >= dominance_threshold].index.tolist()\n",
    "\n",
    "# Não dropar colunas categóricas — variância delas é esperada ser concentrada após encoding\n",
    "cols_to_drop_low_variance = [c for c in cols_to_drop_low_variance if c not in categorical_columns]\n",
    "\n",
    "df = df.drop(columns=cols_to_drop_low_variance)\n",
    "\n",
    "print(f'Colunas removidas (>{dominance_threshold*100:.0f}% mesmo valor): {len(cols_to_drop_low_variance)}')\n",
    "print(cols_to_drop_low_variance)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "665d6a80",
   "metadata": {},
   "source": [
    "### 3.9 Encode target\n",
    "\n",
    "| Class | Meaning | Label |\n",
    "|-------|---------|-------|\n",
    "| 5 | Discarded | 0 |\n",
    "| 10 | Confirmed dengue | 1 |\n",
    "| 11 | Confirmed + alarm signs | 1 |\n",
    "| 12 | Confirmed + severe dengue | 1 |\n",
    "\n",
    "All inconclusive records are excluded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5e1b6e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tratando a coluna de target\n",
    "if type_disease == 'dengue':\n",
    "    df = df[df['final_classification'].isin([5, 10, 11, 12])]\n",
    "    \n",
    "if type_disease == 'chikungunya':\n",
    "    df = df[df['final_classification'].isin([5, 13])]\n",
    "\n",
    "dengue_mapping = {\n",
    "    5 : 0,   # Discarded\n",
    "    10 : 1,  # Confirmed\n",
    "    11 : 1,  # Confirmed and alarming\n",
    "    12 : 1,  # Confirmed with complications\n",
    "}\n",
    "\n",
    "chik_mapping = {\n",
    "    5 : 0,   # Discarded\n",
    "    13 : 1,  # Confirmed Chikungunya\n",
    "}\n",
    "\n",
    "df['final_classification'] = df['final_classification'].map(dengue_mapping if type_disease == 'dengue' else chik_mapping).fillna(0).astype(int)\n",
    "df['final_classification'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db2ea96a",
   "metadata": {},
   "source": [
    "## 4. Model Training — Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a38fa0",
   "metadata": {},
   "source": [
    "### 4.1 Convert to tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a23b09a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_tensors = torch.tensor(df[categorical_columns].values, dtype=torch.long).to(device)\n",
    "numerical_tensors = torch.tensor(df.drop(columns=categorical_columns + ['final_classification']).values, dtype=torch.float).to(device)\n",
    "target_tensor = torch.tensor(df['final_classification'].values, dtype=torch.long).to(device)\n",
    "\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f749da3",
   "metadata": {},
   "source": [
    "### 4.2 Compute embedding sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e2a8b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_cat = [df[col].max() + 1 for col in categorical_columns]\n",
    "embedding_sizes = [(size, min(50, (size // 2) + 1)) for size in unique_cat]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41922fd1",
   "metadata": {},
   "source": [
    "### 4.3 Model architecture — `DengueTabularNN`\n",
    "\n",
    "Tabular neural network with:\n",
    "- **Embeddings** for categorical columns (each category gets a dense vector)\n",
    "- **BatchNorm** for numerical inputs\n",
    "- **4 hidden layers** (1024 → 512 → 256 → 128) with LeakyReLU, BatchNorm, Dropout\n",
    "- **Single logit output** for BCEWithLogitsLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2feae18e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DengueTabularNN(nn.Module):\n",
    "    def __init__(self, numericals_shape, embedding_sizes, hidden_layers = [600, 300, 200, 100], probability_dropout = [0.05, 0.3]):\n",
    "        super().__init__()\n",
    "\n",
    "        # Tratamento dos embeddings\n",
    "        lista_embeddings = [nn.Embedding(size, new_size) for size, new_size in embedding_sizes]\n",
    "        self.embeddings = nn.ModuleList(lista_embeddings)\n",
    "        self.dropout_embeddings = nn.Dropout(p = probability_dropout[0])\n",
    "\n",
    "        # Normalização numéricas\n",
    "        self.normalization = nn.BatchNorm1d(numericals_shape)\n",
    "\n",
    "        # Soma para a entrada da primeira camada pra hidden layer\n",
    "        sum_columns = sum([embedding_sizes[i][1] for i in range(len(embedding_sizes))]) + numericals_shape\n",
    "\n",
    "        layers = []\n",
    "        current_entries = sum_columns\n",
    "        for layer_neurons in hidden_layers:\n",
    "            layers.append(nn.Linear(current_entries, layer_neurons))\n",
    "            layers.append(nn.LeakyReLU())\n",
    "            layers.append(nn.BatchNorm1d(layer_neurons))\n",
    "            layers.append(nn.Dropout(p = probability_dropout[1]))\n",
    "            current_entries = layer_neurons\n",
    "\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "        self.output_layer = nn.Linear(current_entries, 1)\n",
    "\n",
    "    def _engineering_embeddings(self, x_categorical):\n",
    "        embedded = []\n",
    "        for i, embedding in enumerate(self.embeddings):\n",
    "            embedded.append(embedding(x_categorical[:, i]))\n",
    "        return torch.cat(embedded, dim=1)\n",
    "    \n",
    "    def forward(self, x_categorical, x_numerical):\n",
    "        x_categorical = self._engineering_embeddings(x_categorical)\n",
    "        x_categorical = self.dropout_embeddings(x_categorical)\n",
    "        x_numerical = self.normalization(x_numerical)\n",
    "        x = torch.cat([x_categorical, x_numerical], dim=1)\n",
    "        x = self.layers(x)\n",
    "        return self.output_layer(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b836754",
   "metadata": {},
   "source": [
    "### 4.4 Training loop\n",
    "\n",
    "**Setup:** 90/10 train/test split, batch size 4096  \n",
    "**Loss:** BCEWithLogitsLoss with `pos_weight` to handle class imbalance  \n",
    "**Optimizer:** AdamW (lr=1e-4, weight_decay=1e-4)  \n",
    "**Scheduler:** ReduceLROnPlateau (patience=3, factor=0.5)  \n",
    "**Early stopping:** patience=8 on validation loss, saves best checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "476934c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_cat, x_test_cat, x_train_num, x_test_num, y_train, y_test = train_test_split(categorical_tensors, numerical_tensors, target_tensor, test_size=0.1, shuffle=True, random_state=42)\n",
    "\n",
    "train_dataset = TensorDataset(x_train_cat, x_train_num, y_train)\n",
    "train_loader = DataLoader(train_dataset, batch_size=4096, shuffle=True)\n",
    "\n",
    "test_dataset = TensorDataset(x_test_cat, x_test_num, y_test)\n",
    "test_loader = DataLoader(test_dataset, batch_size=4096, shuffle=False)\n",
    "\n",
    "dengue_model = DengueTabularNN(embedding_sizes=embedding_sizes, hidden_layers=[2048, 1024, 512, 256], numericals_shape=x_train_num.shape[1], probability_dropout=[0.1, 0.2]).to(device)\n",
    "pos_weight = (y_train == 0).sum().float() / (y_train == 1).sum().float()\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight.to(device))\n",
    "optimizer = torch.optim.AdamW(params=dengue_model.parameters(), lr = 1e-4, weight_decay=1e-4)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=3, factor=0.5, min_lr=1e-6)\n",
    "epochs = 150\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "patience = 8\n",
    "counter = 0\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    dengue_model.train()\n",
    "    epoch_train_loss = 0\n",
    "    for cat, num, target in train_loader:\n",
    "        cat, num, target = cat.to(device), num.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        pred = dengue_model(cat, num)\n",
    "        loss = criterion(pred, target.unsqueeze(1).float())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_train_loss += loss.item() * len(cat)\n",
    "\n",
    "    avg_train_loss = epoch_train_loss / len(train_dataset)\n",
    "    train_losses.append(avg_train_loss)\n",
    "\n",
    "    dengue_model.eval()\n",
    "    epoch_val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for cat, num, target in test_loader:\n",
    "            cat, num, target = cat.to(device), num.to(device), target.to(device)\n",
    "            pred = dengue_model(cat, num)\n",
    "            loss = criterion(pred, target.unsqueeze(1).float())\n",
    "            epoch_val_loss += loss.item() * len(cat)\n",
    "\n",
    "    avg_val_loss = epoch_val_loss / len(test_dataset)\n",
    "    val_losses.append(avg_val_loss)\n",
    "    scheduler.step(avg_val_loss)\n",
    "\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        counter = 0\n",
    "        torch.save(dengue_model.state_dict(), f'C:\\\\Users\\\\angej\\\\Documents\\\\2_Programação\\\\health_index_project\\\\models_saved\\\\best_{type_disease}_model.pth')\n",
    "    else:\n",
    "        counter += 1\n",
    "        if counter >= patience:\n",
    "            print(f'Early stopping at epoch {epoch}')\n",
    "            break\n",
    "\n",
    "    print(f'Epoch: {epoch:3d} | Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f} | LR: {scheduler._last_lr[0]:.6f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aa0828f",
   "metadata": {},
   "source": [
    "### 4.5 Loss curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14ec9f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style('whitegrid')\n",
    "\n",
    "plt.figure(figsize=(9, 5), dpi = 100)\n",
    "sns.lineplot(x=range(1, len(train_losses) + 1), y=train_losses, label='Train Loss')\n",
    "sns.lineplot(x=range(1, len(val_losses) + 1), y=val_losses, label='Validation Loss')\n",
    "\n",
    "for spine in plt.gca().spines.values(): spine.set_visible(False)\n",
    "plt.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('MSE Loss')\n",
    "plt.legend()\n",
    "plt.title('Training Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56f92100",
   "metadata": {},
   "source": [
    "## 5. Evaluation\n",
    "\n",
    "### 5.1 Threshold sweep — Neural Network\n",
    "\n",
    "Loads the best checkpoint and evaluates Accuracy, Precision, Recall, and F1 across thresholds from 0.30 to 0.60."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f29823",
   "metadata": {},
   "outputs": [],
   "source": [
    "dengue_model = DengueTabularNN(embedding_sizes=embedding_sizes, hidden_layers=[2048, 1024, 512, 256], numericals_shape=x_train_num.shape[1], probability_dropout=[0.1, 0.2]).to(device)\n",
    "dengue_model.load_state_dict(torch.load(f'C:\\\\Users\\\\angej\\\\Documents\\\\2_Programação\\\\health_index_project\\\\models_saved\\\\best_{type_disease}_model.pth', weights_only=True))\n",
    "dengue_model.eval()\n",
    "\n",
    "all_probs = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for X_cat_batch, X_num_batch, _ in test_loader:\n",
    "        y_pred_batch = dengue_model(X_cat_batch, X_num_batch)\n",
    "        all_probs.append(torch.sigmoid(y_pred_batch).squeeze().cpu())\n",
    "\n",
    "probabilities = torch.cat(all_probs)\n",
    "y_true = y_test.cpu()\n",
    "\n",
    "# Busca pelo melhor threshold\n",
    "print(f'{\"Threshold\":>10} | {\"Accuracy\":>10} | {\"Precision\":>10} | {\"Recall\":>10} | {\"F1\":>10}')\n",
    "print('-' * 60)\n",
    "for t in [0.3, 0.35, 0.4, 0.45, 0.5, 0.55, 0.6]:\n",
    "    preds = (probabilities > t).long()\n",
    "    print(f'{t:>10.2f} | {(preds == y_true).float().mean().item():>10.4f} | {precision_score(y_true, preds):>10.4f} | {recall_score(y_true, preds):>10.4f} | {f1_score(y_true, preds):>10.4f}')\n",
    "\n",
    "predicted_classes = (probabilities > 0.3).long()\n",
    "\n",
    "test = pd.DataFrame({\n",
    "    'Actual': y_true.numpy(),\n",
    "    'Prob': probabilities.numpy(),\n",
    "    'Predicted': predicted_classes.numpy(),\n",
    "    'Correct': (y_true == predicted_classes).numpy()\n",
    "})\n",
    "display(test.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d91fe295",
   "metadata": {},
   "source": [
    "### 5.2 Permutation feature importance — Neural Network\n",
    "\n",
    "Wraps the model in a sklearn-compatible interface to compute permutation importance on 2000 test samples (100 repeats)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "652dcedb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subsample first, then concatenate\n",
    "idx = np.random.choice(x_test_cat.shape[0], size=2000, replace=False)\n",
    "\n",
    "X_test = np.concatenate([\n",
    "    x_test_cat[idx].cpu().numpy(),\n",
    "    x_test_num[idx].cpu().numpy().astype(np.float32)\n",
    "], axis=1)\n",
    "y_test_np = y_test[idx].cpu().numpy().astype(int).flatten()\n",
    "\n",
    "n_cat = x_test_cat.shape[1]  # define outside wrapper to avoid closure issues\n",
    "\n",
    "class SklearnWrapper:\n",
    "    def fit(self, X, y):\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        cat = torch.tensor(X[:, :n_cat], dtype=torch.long).to(device)\n",
    "        num = torch.tensor(X[:, n_cat:], dtype=torch.float32).to(device)\n",
    "        with torch.no_grad():\n",
    "            out = dengue_model(cat, num)\n",
    "        return (out.cpu().numpy() > 0.5).astype(int).flatten()\n",
    "\n",
    "    def score(self, X, y):\n",
    "        from sklearn.metrics import accuracy_score\n",
    "        return accuracy_score(y, self.predict(X))\n",
    "\n",
    "wrapper = SklearnWrapper()\n",
    "result = permutation_importance(wrapper, X_test, y_test_np, n_repeats=100, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9190dff2",
   "metadata": {},
   "source": [
    "### 5.3 Top-20 feature importance chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf99e42e",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_n = 20\n",
    "\n",
    "all_feature_names = categorical_columns + list(df.drop(columns=categorical_columns + ['final_classification']).columns)\n",
    "sorted_idx = result.importances_mean.argsort()[::-1][:top_n]\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.bar(range(top_n), result.importances_mean[sorted_idx], yerr=result.importances_std[sorted_idx])\n",
    "plt.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "for spine in plt.gca().spines.values(): spine.set_visible(False)\n",
    "plt.xticks(range(top_n), [all_feature_names[i] for i in sorted_idx], rotation=90)\n",
    "plt.title(f'Permutation Feature Importance - Top {top_n}', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f534a15",
   "metadata": {},
   "source": [
    "## 6. Baseline — LightGBM\n",
    "\n",
    "Trains a gradient-boosted tree model on the same train/test split as a performance baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vnlvtbzh32",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_lgbm = pd.concat([\n",
    "    pd.DataFrame(x_train_cat.cpu().numpy(), columns=categorical_columns),\n",
    "    pd.DataFrame(x_train_num.cpu().numpy(), columns=df.drop(columns=categorical_columns + ['final_classification']).columns)\n",
    "], axis=1)\n",
    "\n",
    "X_test_lgbm = pd.concat([\n",
    "    pd.DataFrame(x_test_cat.cpu().numpy(), columns=categorical_columns),\n",
    "    pd.DataFrame(x_test_num.cpu().numpy(), columns=df.drop(columns=categorical_columns + ['final_classification']).columns)\n",
    "], axis=1)\n",
    "\n",
    "lgbm_model = LGBMClassifier(\n",
    "    n_estimators=2000,\n",
    "    learning_rate=0.03,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    device='gpu',\n",
    ")\n",
    "\n",
    "lgbm_model.fit(X_train_lgbm, y_train.cpu().numpy())\n",
    "\n",
    "lgbm_accuracy = (lgbm_model.predict(X_test_lgbm) == y_test.cpu().numpy()).mean()\n",
    "print(f'LightGBM Accuracy: {lgbm_accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2596bcdb",
   "metadata": {},
   "source": [
    "### 6.1 Feature importance — LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "qm5dl79jd0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = pd.Series(lgbm_model.feature_importances_, index=X_train_lgbm.columns)\n",
    "importances = importances.sort_values(ascending=False)\n",
    "    \n",
    "plt.figure(figsize=(12, 8))\n",
    "importances.head(30).plot(kind='bar')\n",
    "plt.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "for spine in plt.gca().spines.values(): spine.set_visible(False)\n",
    "plt.title('LightGBM - Top 30 Feature Importances')\n",
    "plt.ylabel('Importance (F-score)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(importances.to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38874ada",
   "metadata": {},
   "source": [
    "### 6.2 Threshold sweep — LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10f4af3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy, Precision, Recall e F1 para o modelo XGBoost based on thresholds list\n",
    "\n",
    "for t in [0.1, 0.3, 0.35, 0.4, 0.45, 0.5, 0.55, 0.6]:\n",
    "    xgb_probs = lgbm_model.predict_proba(X_test_lgbm)[:, 1]\n",
    "    xgb_preds = (xgb_probs > t).astype(int)\n",
    "    print(f'Threshold: {t:.2f} | Accuracy: {(xgb_preds == y_test.cpu().numpy()).mean():.4f} | Precision: {precision_score(y_test.cpu().numpy(), xgb_preds):.4f} | Recall: {recall_score(y_test.cpu().numpy(), xgb_preds):.4f} | F1: {f1_score(y_test.cpu().numpy(), xgb_preds):.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
