{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "94443428",
   "metadata": {},
   "source": [
    "## 1. Setup & Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4b46dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "from pathlib import Path\n",
    "sys.path.append(str(Path.cwd().parent))\n",
    "\n",
    "from data_processing.disease_dataset_process import DataProcessor, CATEGORICAL_COLUMNS\n",
    "from models_classes.mlp_disease_neural_net import DengueTabularNN\n",
    "from models_classes.lgbm_classifier import LGBMDiseaseClassifier\n",
    "\n",
    "print(f'PyTorch: {torch.__version__}')\n",
    "print(f'CUDA built with: {torch.version.cuda}')\n",
    "print(f'CUDA available: {torch.cuda.is_available()}')\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dqyd6ipgsqg",
   "source": "## 2. Data Loading & Feature Engineering\n\n`DataProcessor` handles loading the raw CSVs, renaming columns, dropping leaky/irrelevant fields, deriving date features, encoding categorical columns, binarizing symptoms/comorbidities, and mapping the target — all in one call.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71dbf879",
   "metadata": {},
   "outputs": [],
   "source": [
    "type_disease = 'chikungunya'  # Change to 'dengue' for Dengue dataset\n",
    "\n",
    "data_processor = DataProcessor(type_disease=type_disease)\n",
    "df, categorical_columns, _ = data_processor.load_data_process()\n",
    "numerical_columns = df.drop(columns=categorical_columns + ['final_classification']).columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a38fa0",
   "metadata": {},
   "source": "## 3. Model Training — Neural Network\n\n### 3.1 Convert to tensors and split"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a23b09a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_tensors, numerical_tensors, target_tensor, embedding_sizes = DengueTabularNN._prepare_data(df, categorical_columns)\n",
    "x_train_cat, x_test_cat, x_train_num, x_test_num, y_train, y_test = train_test_split(categorical_tensors, numerical_tensors, target_tensor, test_size=0.1, shuffle=True, random_state=42)\n",
    "\n",
    "print(f'Train size: {len(y_train)} | Test size: {len(y_test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f749da3",
   "metadata": {},
   "source": "### 3.2 Instantiate model"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e2a8b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dengue_model = DengueTabularNN(numericals_shape=x_train_num.shape[1], embedding_sizes=embedding_sizes, hidden_layers=[2048, 1024, 512, 256], probability_dropout=[0.1, 0.2]).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41922fd1",
   "metadata": {},
   "source": "### 3.3 Model architecture — `DengueTabularNN`\n\nTabular neural network with:\n- **Embeddings** for categorical columns (each category gets a dense vector)\n- **BatchNorm** for numerical inputs\n- **4 hidden layers** (2048 → 1024 → 512 → 256) with LeakyReLU, BatchNorm, Dropout\n- **Single logit output** for BCEWithLogitsLoss"
  },
  {
   "cell_type": "markdown",
   "id": "6b836754",
   "metadata": {},
   "source": "### 3.4 Training loop\n\n**Setup:** 90/10 train/test split, batch size 4096  \n**Loss:** BCEWithLogitsLoss with `pos_weight` to handle class imbalance  \n**Optimizer:** AdamW (lr=1e-4, weight_decay=1e-4)  \n**Scheduler:** ReduceLROnPlateau (patience=3, factor=0.5)  \n**Early stopping:** patience=8 on validation loss, saves best checkpoint"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "476934c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TensorDataset(x_train_cat, x_train_num, y_train)\n",
    "train_loader = DataLoader(train_dataset, batch_size=4096, shuffle=True)\n",
    "\n",
    "test_dataset = TensorDataset(x_test_cat, x_test_num, y_test)\n",
    "test_loader = DataLoader(test_dataset, batch_size=4096, shuffle=False)\n",
    "\n",
    "pos_weight = (y_train == 0).sum().float() / (y_train == 1).sum().float()\n",
    "criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight.to(device))\n",
    "optimizer = torch.optim.AdamW(params=dengue_model.parameters(), lr=1e-4, weight_decay=1e-4)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=3, factor=0.5, min_lr=1e-6)\n",
    "\n",
    "epochs = 150\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "patience = 8\n",
    "counter = 0\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    dengue_model.train()\n",
    "    epoch_train_loss = 0\n",
    "    for cat, num, target in train_loader:\n",
    "        cat, num, target = cat.to(device), num.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        pred = dengue_model(cat, num)\n",
    "        loss = criterion(pred, target.unsqueeze(1).float())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_train_loss += loss.item() * len(cat)\n",
    "\n",
    "    avg_train_loss = epoch_train_loss / len(train_dataset)\n",
    "    train_losses.append(avg_train_loss)\n",
    "\n",
    "    dengue_model.eval()\n",
    "    epoch_val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for cat, num, target in test_loader:\n",
    "            cat, num, target = cat.to(device), num.to(device), target.to(device)\n",
    "            pred = dengue_model(cat, num)\n",
    "            loss = criterion(pred, target.unsqueeze(1).float())\n",
    "            epoch_val_loss += loss.item() * len(cat)\n",
    "\n",
    "    avg_val_loss = epoch_val_loss / len(test_dataset)\n",
    "    val_losses.append(avg_val_loss)\n",
    "    scheduler.step(avg_val_loss)\n",
    "\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        counter = 0\n",
    "        torch.save(dengue_model.state_dict(), f'C:\\\\Users\\\\angej\\\\Documents\\\\2_Programação\\\\health_index_project\\\\models_saved\\\\best_{type_disease}_model.pth')\n",
    "    else:\n",
    "        counter += 1\n",
    "        if counter >= patience:\n",
    "            print(f'Early stopping at epoch {epoch}')\n",
    "            break\n",
    "\n",
    "    print(f'Epoch: {epoch:3d} | Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f} | LR: {scheduler._last_lr[0]:.6f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aa0828f",
   "metadata": {},
   "source": "### 3.5 Loss curves"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14ec9f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style('whitegrid')\n",
    "\n",
    "plt.figure(figsize=(9, 5), dpi = 100)\n",
    "sns.lineplot(x=range(1, len(train_losses) + 1), y=train_losses, label='Train Loss')\n",
    "sns.lineplot(x=range(1, len(val_losses) + 1), y=val_losses, label='Validation Loss')\n",
    "\n",
    "for spine in plt.gca().spines.values(): spine.set_visible(False)\n",
    "plt.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('MSE Loss')\n",
    "plt.legend()\n",
    "plt.title('Training Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56f92100",
   "metadata": {},
   "source": "## 4. Evaluation\n\n### 4.1 Threshold sweep — Neural Network\n\nLoads the best checkpoint and evaluates Accuracy, Precision, Recall, and F1 across thresholds from 0.30 to 0.60."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f29823",
   "metadata": {},
   "outputs": [],
   "source": [
    "dengue_model.load_state_dict(torch.load(f'C:\\\\Users\\\\angej\\\\Documents\\\\2_Programação\\\\health_index_project\\\\models_saved\\\\best_{type_disease}_model.pth',weights_only=True))\n",
    "display(dengue_model.evaluate(test_loader, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d91fe295",
   "metadata": {},
   "source": "### 4.2 Permutation feature importance — Neural Network\n\nSubsamples 2000 test records and computes permutation importance (100 repeats) via a sklearn-compatible wrapper inside `DengueTabularNN.plot_feature_importance`."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf99e42e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dengue_model.plot_feature_importance(x_test_cat, x_test_num, y_test, categorical_columns, numerical_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f534a15",
   "metadata": {},
   "source": "## 5. Baseline — LightGBM\n\nTrains a gradient-boosted tree model on the same train/test split as a performance baseline."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vnlvtbzh32",
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm = LGBMDiseaseClassifier()\n",
    "lgbm.fit(x_train_cat, x_train_num, y_train, categorical_columns, numerical_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2596bcdb",
   "metadata": {},
   "source": "### 5.1 Feature importance — LightGBM"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "qm5dl79jd0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm.plot_feature_importance(top_n=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38874ada",
   "metadata": {},
   "source": "### 5.2 Threshold sweep — LightGBM"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10f4af3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm.evaluate(x_test_cat, x_test_num, y_test, categorical_columns, numerical_columns)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.14.2)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}